---
title: "Interpretation of Generic Language is Depends on Listener's Backgroud Knowledge"
bibliography: generic-statement.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
 \author{Xiuyuan Zhang \and Daniel Yurovsky \\
         \texttt{\{xiuyuanzhang, yurovsky\}@uchicago.edu} \\
        Department of Psychology \\ University of Chicago}

abstract: >
    Generic language, like "birds lay eggs" or "dogs bark" are simple and ubiquitou in naturally produced speech. However, the inherent vagueness of generics makes their interpretation highly context dependent. Building on work by @tessler2019 showing that generics can be thought of as inherently relative (i.e. more birds lay eggs than you would expect), we explore the consequences of different implied comparison categories on the interpretaiton of novel generics. In Experiment 1, we manipulated the set of categories salient to a listener by directly providing them the comparison sets. In Experiment 2, we collected participantsâ€™ demographic information and used these naturally occurring differences as a basis for differences in the participants' comparison sets. Results from both studies confirmed our hypothesis that the prevalence of a feature in different comparison categories changes people' estimatite of the feature prevalence in novel categories. ONE MORE SENTENCE HERE ABOUT IMPLICATIONS
    
keywords: >
    generics; semantics; meaning; learning; Bayesian inference
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(jpeg)
library(grid)
library(xtable)

library(tidyverse)
library(lubridate)
library(lme4)
library(lmerTest)
library(knitr)
library(tidyboot)
library(ggrepel)
library(ggridges)
library(here)
library(broom)
library(broom.mixed)
library(viridis)
library(papaja)
library(ggthemes)
theme_set(theme_classic())
```

# Introduction

Generic language like "birds lay eggs" is a simple, highly frequent way of transmitting information in everyday speech [@gelman1998;@gelman2008]. Generics are distinct from statements about particular referents "e.g. that bird lays eggs"; they transmit information about *categories*. Indeed, a large body of research has documented the power of generic language in adults' and children's inference about familiar and novel categories [e.g.,@cimpian2010;@cimpian2011;@rhodes2012]. Despite their ubiquity, generic statements defy a straightforward definition in threshold semantics (i.e. they do not specify a fixed prevalence rate). While people generally agree that "birds lay eggs," this does not mean that "all birds lay eggs ($100\%$)" nor does it mean that "most birds lay eggs ($>50\%$)"--male birds, and young female birds do not. Similarly, "birds lay eggs" cannot mean "some birds lay eggs ($>0\%$)," because it is true while "birds are female" is not.

Recent work from @tessler2019 shows that generics can be understood through the lens of Gricean pragmatic inference [@grice1975]. Their key insight is that generics can be interpreted as statements about relative prevalence. If a speaker makes a vague statement like "birds lay eggs," but listeners assume that they are cooperatively intending to be informative, they can infer that the speaker means something like "birds are *more likely than you would have expected* to lay eggs." This formulation leaves open two questions: (1) how much more likely does a speaker mean, and (2) what did the listener expect? Tessler and Goodman answer the first question by showing that listeners do not need to resolve this ambiguity directly, but can instead integrate over all prevalence rates that would make the speaker's statement true. In a series of experiments with both familiar and novel generics, @tessler2019 show that people's judgements about prevalence rates following a generic statement are described by a rational model  pragmatic inference [@frank2012]. 

We take up the second question: How do listeners arrive at their prior expectations? One possibility is that implicit in a generic statement is a set of reference categories, i.e. "birds lay eggs" means "relative to relevant comparison categories, birds are more likely to lay eggs." The listener's interpretation of a generic, then, should depend on the set of categories they consider relevant. That is, "feps are friendly (relative to puppies)" should lead to a much different estimate of the prevalence of friendliness in feps than "feps are friendly (relative to squirrels)." 

We test this prediction in a two experiments in which people learn about novel categories through generic language. In the first, we manipulate the implied comparison category directly and show that people's judgments about the prevalence of a feature in a novel category tracks the prevalence level of the implied category. In a second experiment, we show the influence of implicit comparison categories without manipulating them. Here we leverage prior work showing that people's estimates about the prevalence of preferences and beliefs in others are egocentrically biased towards the prevalence of those preferences and beliefs in their local communities [@ross1977]. Together, these studies highlight the fundamentally relative way in which even simple generic statements are interpreted, and point towards a potential source of misunderstanding and errors in learning that can arise from well-intentioned communication. 

# Experiment 1

Two conditions were tested in this experiment - (1a) a baseline survey, where we asked participants to estimate prevalence rate for familiar category-feature pairs, and (1b) a novel category survey, where participants were introduced to a novel category along with a familiar comparison category, then they were shown a generic containing a novel category and a familiar feature and asked to estimate the prevalence rate of the feature within the novel category. 


```{r e1_data}
#### read in data
### baseline ###
data_baseline <- read_csv(here("baseline-survey110218/data/020818baseline-anonymized-data.csv"))
### explicit generic ###
data_generic <- read_csv(here("generic-survey110218/data/020818generic-anonymized-data.csv"))

#### filter participants who passed attention check 
### baseline ###
qualtrics_filtered_baseline <- data_baseline %>%
  filter(Q8 == "friendly,tasty,heavy")
### explicit generic ###
qualtrics_filtered_generic <- data_generic %>%
  filter(Q10 == "kobas,feps,dands")

#### Munge data
### In both surveys, the categories and features we chose are consistent.
# use to group different levels
high_objects <- c("puppies","trucks","pizzas")
med_objects <- c("goats", "rocks", "fruits")
low_objects <- c("squirrels", "bikes", "vegetables")
features <- c("friendly", "heavy", "tasty")

# features and objects for the survey
objects <- data_frame(type = "high", object = high_objects,
                      feature = features) %>%
  bind_rows(data_frame(type = "medium", object = med_objects,
                       feature = features)) %>%
  bind_rows(data_frame(type = "low", object = low_objects,
                       feature = features))

# get data for each of the three conditions
### baseline ###
q1_baseline <- qualtrics_filtered_baseline %>%
  select(id, L_feature, L_object, Q1, Q2_1) %>%
  rename(feature = L_feature, baseline_object = L_object, truefalse_response = Q1, percent_response = Q2_1)

q2_baseline <- qualtrics_filtered_baseline %>%
  select(id, M_feature, M_object, Q3, Q4_1) %>%
  rename(feature = M_feature, baseline_object = M_object, truefalse_response = Q3, percent_response = Q4_1)

q3_baseline <- qualtrics_filtered_baseline %>%
  select(id, H_feature, H_object, Q5, Q6_1) %>%
  rename(feature = H_feature, baseline_object = H_object, truefalse_response = Q5, percent_response = Q6_1)

### explicit generic ###
colnames(qualtrics_filtered_generic)[colnames(qualtrics_filtered_generic) == "Duration (in seconds)"] <- "time_spent"

# get data for each of the three conditions
q1_generic <- qualtrics_filtered_generic %>%
  select(id, L_feature, L_comparison, L_novel, Q2_1, time_spent) %>%
  rename(feature = L_feature, baseline_object = L_comparison, novel_object = L_novel, percent_response = Q2_1)

q2_generic <- qualtrics_filtered_generic %>%
  select(id, M_feature, M_comparison, M_novel, Q5_1, time_spent) %>%
  rename(feature = M_feature, baseline_object = M_comparison, novel_object = M_novel,percent_response = Q5_1)

q3_generic <- qualtrics_filtered_generic %>%
  select(id, H_feature, H_comparison, H_novel, Q6_1, time_spent) %>%
  rename(feature = H_feature, baseline_object = H_comparison, novel_object = H_novel, percent_response = Q6_1)

# combine
### baseline ###
response_data_baseline <- bind_rows(q1_baseline, q2_baseline, q3_baseline) %>%
  mutate(type = if_else(baseline_object %in% high_objects, "high",
                        if_else(baseline_object %in% med_objects, "medium","low"))) %>%
  mutate(type = factor(type, levels = c("low", "medium", "high")))
### explicit generic ###
response_data_generic <- bind_rows(q1_generic, q2_generic, q3_generic) %>%
  mutate(type = if_else(baseline_object %in% high_objects, "high",
                        if_else(baseline_object %in% med_objects, "medium","low"))) %>%
  mutate(type = factor(type, levels = c("low", "medium", "high")))

#### Plot for CogSci
exp1_old_long_data <- response_data_baseline %>%
  mutate(condition = "baseline")

exp1_new_long_data <- response_data_generic %>%
  mutate(condition = "novel")
  
exp1_all_long_data <- bind_rows(exp1_new_long_data, exp1_old_long_data) %>%
  select(-truefalse_response, -time_spent)

exp1_empirical_stats <- exp1_all_long_data %>%
  group_by(condition, type) %>%
  tidyboot_mean(percent_response) 
```

## Method

### Participants 
A total of `r nrow(data_baseline) + nrow(data_generic)` participants were recruited on Amazon Mechnical Turk, `r nrow(data_baseline)` participated in Experiment 1a, and `r nrow(data_baseline)` participated in Experiment 1b. Each participant gave informed consent at the start of the Experiment and was paid **AMOUNT** in compensation. Participants were excluded from the final sample if they did not pass an attention check at the end of experiment (`r nrow(data_baseline) - nrow(qualtrics_filtered_baseline)` in Experiment 1a, and `r nrow(data_generic) - nrow(qualtrics_filtered_generic)` in Experiment 1b) yielding a final sample of `r nrow(qualtrics_filtered_baseline)` participants in 1a and `r nrow(qualtrics_filtered_generic)` participants in 1b.

### Design and Procedure

Three features (friendly, tasty, and heavy) were chosen, and, for each feature, we chose three categories that were relevant to the feature that will elicit different levels(low, medium, high) of prevalence estimation. Every participant answered questions about each of three features and, for each feature, one randonly-selected category. The order in which the features appeared in the survey was randomized, and each participant was tested on only one category from each of the predetermined prevalence levels.

In Experiment 1a, participants were first shown a generic for each of the category and feature pair. Then they were asked to first evaluate the truth condition of the generic by answering a forced-choice True or False question, and to estimate the proportion of the feature within the given category. Participants recorded their responses to the estimation question on a scale slider, ranging from 0% to 100%.

In Experiment 1b, participants answered similar questions but about novel categories. Participants were first told that they are visiting three new countries, and people from there will introduce them to things in their respective countries. Each novel category was first introduced by reference to one of the baseline categories from Experiment 1a (e.g. "Feps are like puppies"). Then, they were given a novel generic using the same features about this novel category (e.g., "Feps are friendly"). All other aspects of the design were identical.

After completing these questions, participants were given an attention check to ensure that they had read and engaged with the stimuli. In Experiment 1a., we asked participants to choose the three features we asked about in the survey. In 1b., we asked participants to select the three novel categories that were mentioned in the survey.

## Results and Analysis

```{r exp1_fig, fig.env="figure", fig.pos = "tb", fig.align = "center", fig.width=3, fig.height=2.5, fig.cap = "Prevalence judgments of participants in the Baseline (1a) and Novel (1b) conditions. Error bars indicate 95\\% confidence intervals computed by nonparametric bootstrapping", cache = T}
exp1_empirical_stats %>% 
  ggplot(aes(x = type, y = empirical_stat, fill = condition)) +
  geom_bar(stat="identity", position=position_dodge()) +
  geom_linerange(aes(ymin=ci_lower, ymax=ci_upper), position = position_dodge(.9))+
  scale_fill_ptol() +
  xlab("Predicted prevalence Level") +
  ylab("Prevalence judgment (%)") +
  theme(legend.position = c(.2,.9), legend.title = element_blank())
  #labs(title = "Estimation Responses to Selected Questions Across Features", subtitle = "Respective comparison category assigned to participants in the novel condition") + 
  # theme(axis.title.x =element_text(size=8, hjust = 0.5),
  #       axis.title.y =element_text(size=8, hjust = 0.5),
  #       legend.text = element_text(size=6),
  #       legend.title = element_text(size = 8))
```

```{r}
exp1_lmer <- exp1_all_long_data %>%
  mutate(id = paste0(condition, "_", id)) %>%
  lmer(percent_response/100 ~ type * condition + (1|id) + (1|feature), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group)
```

Figure \ref{fig:exp1_fig} shows participants' mean prevalence judgments across conditions in both conditions. In both Experiments, participants' judgments about the prevalence of the target feature in each category increased as predicted from low to medium to high. In addition, judgments made about the novel category were on average higher than the judgements for the corresponding category made by participants in the baseline condition, although this difference was not apparent in the high prevalence condition. We confirmed these predictions statistically using a mixed-effects logistic regression, predicting participants' judgments from condition and prevalence level and their interaction, with random effects of subject and feature (\texttt{prop $\sim$ condition * level + (1|subj) + (level|feature)}). This model revealed a significant effect of level, with both medium ($\beta =$ `r exp1_lmer %>% filter(term == "typemedium") %>% pull(estimate) %>% round(2)`, $t =$ `r exp1_lmer %>% filter(term == "typemedium") %>% pull(statistic) %>% round(2)`, $p$ `r exp1_lmer %>% filter(term == "typemedium") %>% pull(p.value) %>% printp()`) and high ($\beta =$ `r exp1_lmer %>% filter(term == "typehigh") %>% pull(estimate) %>% round(2)`, $t =$ `r exp1_lmer %>% filter(term == "typehigh") %>% pull(statistic) %>% round(2)`, $p$ `r exp1_lmer %>% filter(term == "typehigh") %>% pull(p.value) %>% printp()`) levels of apriori prevalence receiving higher prevalence judgments, a significant effect of condition ($\beta =$ `r exp1_lmer %>% filter(term == "conditionnovel") %>% pull(estimate) %>% round(2)`, $t =$ `r exp1_lmer %>% filter(term == "conditionnovel") %>% pull(statistic) %>% round(2)`, $p$ `r exp1_lmer %>% filter(term == "conditionnovel") %>% pull(p.value) %>% printp()`), and a significant interaction between the two for both medium (($\beta =$ `r exp1_lmer %>% filter(term == "typemedium:conditionnovel") %>% pull(estimate) %>% round(2)`, $t =$ `r exp1_lmer %>% filter(term == "typemedium:conditionnovel") %>% pull(statistic) %>% round(2)`, $p =$ `r exp1_lmer %>% filter(term == "typemedium:conditionnovel") %>% pull(p.value) %>% printp()`) and high levels ($\beta =$ `r exp1_lmer %>% filter(term == "typehigh:conditionnovel") %>% pull(estimate) %>% round(2)`, $t =$ `r exp1_lmer %>% filter(term == "typehigh:conditionnovel") %>% pull(statistic) %>% round(2)`, $p$ `r exp1_lmer %>% filter(term == "typehigh:conditionnovel") %>% pull(p.value) %>% printp()`) indicating that the change in prevalence levels was largest for the lowest apriori level.

## Discussion

**NOT THE REAL TEXT WE WANT**
We found the effect. Maybe some ceiling effects on prevalence? This is great because it shows we can manipulate the effects, but it's pretty explicit. Next we show we can do it with apriori estimates. First we get baseline estimates for demographic groups, then we show that these predict judgments about novel generics.


# Experiment 2
## Method
Experiment 2 is composed of two experiments, where experiment 2a includes a baseline survey of 15 questions about people's habits, and a simulation to select a smaller set of 6 questions to in experiment 2b. Participants were asked to make estimations on the prevalence rate of each habit among people. Experiment 2b used the results from the previous simulation to finalize the 6 questions to be asked. In experiment 2b, participants were introducted to six novel countries and people in those countries, and were asked to make prevalence estimations for each habit among people in the foreign countries. In both experiments, we collected data on participants demographic information, including their gender, age, political ideology score, and zip code. We later used the demographic information to analyze the naturally occuring differences in particpants' comparison sets when making prevalence estimations.

\noindent \textbf{Participants} For Experiment 2a. baseline survey, we recruited 968 adult participants from Amazon Mechanical Turk. 726 participants (Mage = 37 years, SD = 11.92) passed the final attention check and their responses were included in the following analysis. For Experiment 2b novel category survey, 400 adult participants were recruited from Amazon Mechanical Turk. 317 participants (Mage = 35 years, SD = 11.47) passed the final attention check and their responses were included in the analysis. Participants from both conditions were asked a series of demographic questions, including gender, age, political ideology score, and zip code. Participants recruited were U.S. citizens over 18 years old and received monetary compensation for their work. 

\noindent \textbf{Exp 2a. Baseline Survey} Participants in this condition were shown a series of 15 questions. Each question asked them to make an estimate the percentage of people having a certain habit (see Table. 3). Participants responded by choosing a number from 0 - 100% using a slider bar. The order in which the questions appeared was randomized. Six questions were chosen from the previous fifteen questions in Experiment 2a to be asked in Experiment 2b. The selection was based on running simulation (n = 100) on baseline survey responses from participants to all 15 questions and we selected a combination of 6 questions that has large t values. 

```{r Exp2-map, fig.pos = "H", fig.align='center', fig.width=3, fig.heighti=3, set.cap.width=T, fig.cap = "Novel category survey map prompt"}

img_map <- readJPEG(here("cogsci_paper/figs/novel_country_map3.jpeg"))
grid::grid.raster(img_map)

```

\noindent \textbf{Exp 2b. Novel Category Survey} Participants in this condition were first shown a map of 6 imaginary countries and their corresponding novel names (see Fig. 2) along with a prompt. They were told that people from these six countries will introduce some habits of people from their countries to them. They were then introduced to generic statements about each of the country and a habit selected from the simulation. After reading the generic statement, participants were then asked to make an estimate the percentage of people having a certain habit (see Table. 3). Participants responded by choosing a number from 0 - 100% using a slider bar. The order in which the questions appeared was randomized. The code for analyze this experiment is preregistered.

## Results
We calculated the pearson correlation of average mean responsess across demographic groups between experiment 2a. baseline and 2b. novel for all 6 questions selected via simulation. The average mean responses between the two experiments are positively correlated for each question, with p-value of the t-test equals to 0.002. The highest correlation among the six questions was 0.688 for the feature "like to cook at home", and the lowest correlation was 0.216 for "go to the gym". We further examinzed the within-demographic group differences in mean responses between the baseline and novel experiments by questions (see Fig.2). We divided the demographic groups into 8 subgroups (2 age, 2 gender, 2 political leaning). The bins for dividing age and political ideology scores in experiment (2a) were based on the mean age and mean political ideology scores in experiment (2b). Each dot in the sub-plot in Fig.2 is a unique demographic group. Across all six questions, the mean responses from each of the 8 demographic groups for the novel conditions (shown in y-axis) were consistently higher than the mean responses for the baseline condition. The results from experiment 2 showed that participants from different demographic groups make different prevalence estimations within each question, and their responses were on average higher for novel conditions than for baseline conditions. Participants across demographic groups also respond to different questions differently. 



```{r Exp2-mean, fig.env = "figure*", fig.pos = "h", fig.width=6, fig.height=6, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Novel vs. baseline category mean response"}

data_new <- read_csv(here("/demographic_generic/novel_generic/data/01102019_people_novel_habit_anonymized_data.csv"))
data_old <- read_csv(here("/demographic_generic/baseline-people-habits/data/10312018_people_habit_anonymized_data.csv"))

# filter by condition: passing attention check
old_attnCheckAnswers <- c('drink coffee','cook at home', 'go to the gym', 'drive to work')

old_qualtrics_filtered <- data_old %>%
    rowwise() %>%
  mutate(attention_check = (strsplit(attention_check, ','))) %>%
  mutate(attnTrue = sum(attention_check %in% old_attnCheckAnswers),
         attnFalse= sum(!attention_check %in% old_attnCheckAnswers))%>%
  filter(attnFalse==0)

old_tidy_data <- old_qualtrics_filtered %>%
  filter(gender != "Other/Non-conforming") %>%
  mutate(gender = factor(gender, levels = c("Female", "Male"), labels = c("female", "male")))

old_filtered_data <- old_tidy_data %>%
  filter(age < 60) %>%
  mutate(age_bin = if_else(age > median(old_tidy_data$age), "older", "younger"),
         politics_bin = if_else(politics_1 > median(old_tidy_data$politics_1), 
                                "conservative", "liberal"))

new_attnCheckAnswers <- c('like big cities','cook at home', 'go to the gym', 'consume dairy products')

new_qualtrics_filtered <- data_new %>%
    rowwise() %>%
  mutate(attention_check = (strsplit(attention_check, ','))) %>%
  mutate(attnTrue = sum(attention_check %in% new_attnCheckAnswers),
         attnFalse= sum(!attention_check %in% new_attnCheckAnswers))%>%
  filter(attnTrue ==4 & attnFalse==0)

new_tidy_data <- new_qualtrics_filtered %>%
  filter(gender != "Other/Non-conforming") %>%
  mutate(gender = factor(gender, levels = c("Female", "Male"), labels = c("female", "male"))) 

new_filtered_data <- new_tidy_data %>%
  filter(age < 60) %>%
  mutate(age_bin = if_else(age > median(old_tidy_data$age), "older", "younger"),
         politics_bin = if_else(politics_1 > median(old_tidy_data$politics_1), 
                                "conservative", "liberal"))

old_long_data <- old_filtered_data %>%
  select(big_cities_1, computer_preference_1, cook_at_home_1,own_homes_1, dairy_products_1, go_to_gym_1, gender, age_bin, politics_bin, id) %>%
  gather(question, response, big_cities_1, computer_preference_1, cook_at_home_1,own_homes_1, dairy_products_1, go_to_gym_1)

new_long_data <- new_filtered_data %>%
  select(big_cities_1:go_to_gym_1, 
         gender, age_bin, politics_bin, id)%>%
  gather(question, response, big_cities_1:go_to_gym_1)

#### cor
new_data <- new_long_data %>% 
  group_by(id) %>%
  mutate(response = scale(response)) %>%
  group_by(gender, age_bin, politics_bin, question) %>%
  summarise(mean = mean(response, na.rm = T)) %>%
  mutate(data = "new")

old_data <- old_long_data %>%
  group_by(id) %>%
  mutate(response = scale(response)) %>%
  group_by(gender, age_bin, politics_bin, question) %>%
  summarise(mean = mean(response)) %>%
  mutate(data = "old")

all_data <- bind_rows(new_data, old_data) %>%
  group_by(question) %>%
  spread(data, mean) %>%
  summarise(cor = cor(new, old, use = "complete"))

t_test_cor_result <- t.test(all_data$cor)

# UNSCALED
unscaled_new_data <- new_long_data %>% 
  group_by(id) %>%
  group_by(gender, age_bin, politics_bin, question) %>%
  summarise(mean = mean(response, na.rm = T)) %>%
  mutate(data = "new")

unscaled_old_data <- old_long_data %>%
  group_by(id) %>%
  group_by(gender, age_bin, politics_bin, question) %>%
  summarise(mean = mean(response)) %>%
  mutate(data = "old")

unscaled_all_data <- bind_rows(unscaled_new_data, unscaled_old_data) %>%
  group_by(question) %>%
  spread(data, mean) %>%
  summarise(cor = cor(new, old, use = "complete"))

unscaled_all_data_mean <- bind_rows(unscaled_new_data, unscaled_old_data) %>%
  group_by(question) %>%
  spread(data, mean)

question_labels <- c(big_cities_1 = "like big cities",
                     computer_preference_1 = "prefers Macs over PCs",
                     cook_at_home_1 = "like to cook at home",
                     dairy_products_1 = "consume dairy products",
                     go_to_gym_1 = "go to the gym",
                     own_homes_1 = "own homes")

unscaled_all_data_mean %>%
  mutate(demographics = paste(gender, age_bin, politics_bin, sep = ", \n")) %>%
  ggplot(aes(x = old, y = new, colour = question, label = demographics)) +
  geom_jitter(size = 1) +
  geom_text_repel(size = 2, force = 10, point.padding = 0.1, box.padding = 0.3, show.legend = F, segment.alpha = 0.4) +
  coord_fixed(ratio=1, xlim = c(20, 100), ylim = c(20, 100)) +
  geom_abline(intercept = 0, slope = 1, alpha = 0.4) + 
  facet_wrap(~question, labeller = as_labeller(question_labels)) +
  scale_colour_discrete(guide = F) +
  ylab("mean response from novel generics") + 
  xlab("mean response from baseline") +
  labs(title= "Mean Response of Prevalence Estimation for Questions", subtitle = "What proportion of people(novel category) do you think _______?") +
  theme(axis.title.x =element_text(size=8, hjust = 0.5),
        axis.title.y =element_text(size=8, hjust = 0.5),
        strip.text.x = element_text(size = 6),
        plot.title = element_text(size = 8),
        plot.subtitle = element_text(size = 6))


unscaled_all_data_mean %>%
 # filter(quesition %in% c( "go_to_gym_1", "own homes")
  mutate(demographics = paste(gender, age_bin, politics_bin, sep = ", \n")) %>%
  mutate(plot_demographics = if_else(demographics %in% c("male, \nyounger, \nliberal", "female, \nolder, \nconservative"), demographics, "")) %>%
  ggplot(aes(x = old, y = new, colour = question, label = plot_demographics, group = question)) +
  geom_jitter(size = 1) +
  geom_text_repel(size = 2, force = 10, point.padding = 0.1, box.padding = 0.3, show.legend = F, segment.alpha = 0.4) +
  #coord_fixed(ratio=1, xlim = c(20, 100), ylim = c(20, 100)) +
  scale_x_continuous(limits = c(30, 90)) + 
  scale_y_continuous(limits = c(30, 90)) +
  geom_abline(intercept = 0, slope = 1, color = "lightgray", linetype = "dashed") + 
  geom_smooth(method = "lm", se = F) + 
  #facet_wrap(~question, labeller = as_labeller(question_labels)) +
  scale_colour_discrete(guide = F) +
  ylab("mean response from novel generics") + 
  xlab("mean response from baseline") +
  labs(title= "Mean Response of Prevalence Estimation for Questions", subtitle = "What proportion of people(novel category) do you think _______?") +
  theme(axis.title.x =element_text(size=8, hjust = 0.5),
        axis.title.y =element_text(size=8, hjust = 0.5),
        strip.text.x = element_text(size = 6),
        plot.title = element_text(size = 8),
        plot.subtitle = element_text(size = 6))
```



```{r Exp2-ridges, fig.env = "figure*", fig.pos = "h", fig.width=6, fig.height=3, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Novel Condition Prevalence Estimation by Demographic Groups"}

new_long_data%>%
  filter(question %in% c("go_to_gym_1", "own_homes_1")) %>%
  mutate(demographics = paste(gender, age_bin, politics_bin, sep = ",")) %>%
  mutate(demographics = fct_reorder(demographics, response)) %>%
  ggplot(aes(y=demographics, x=response, fill=demographics)) +
  #stat_density_ridges(geom = "density_ridges_gradient", calc_ecdf = TRUE) +
  #scale_fill_viridis(name = "Tail probability", direction = -1)
  geom_density_ridges(alpha=0.6, bandwidth=6, scale=1.5) +
  scale_fill_viridis(discrete=TRUE) +
  scale_color_viridis(discrete=TRUE) +
  xlab("Responses (%)") +
  ylab("Density") +
  facet_wrap(~question, labeller=labeller(question = question_labels)) +
  labs(title= "Participants' Response to Novel Generic Questions by Demographic Subgroup") +
  theme(axis.title.x =element_text(size=8, hjust = 0.5),
        axis.title.y =element_text(size=8, hjust = 0.5),
        strip.text.x = element_text(size = 6),
        plot.title = element_text(size = 8),
        legend.position="none",
        panel.spacing = unit(0.3, "lines"),
        strip.text.y = element_text(size = 6))
```



# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
