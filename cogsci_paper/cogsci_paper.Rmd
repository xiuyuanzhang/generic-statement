---
title: "Interpretation of Generic Language is Depends on Listener's Background Knowledge"
bibliography: generic-statement.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
 \author{Xiuyuan Zhang \and Daniel Yurovsky \\
         \texttt{\{xiuyuanzhang, yurovsky\}@uchicago.edu} \\
        Department of Psychology \\ University of Chicago}

abstract: >
    Generic language, like "birds lay eggs" or "dogs bark" are simple and ubiquitous in naturally produced speech. However, the inherent vagueness of generics makes their interpretation highly context-dependent. Building on work by @tessler2019 showing that generics can be thought of as inherently relative (i.e. more birds lay eggs than you would expect), we explore the consequences of different implied comparison categories on the interpretation of novel generics. In Experiments 1 and 2, we manipulated the set of categories salient to a listener by directly providing them the comparison sets. In Experiments 3 and 4, we collected participantsâ€™ demographic information and used these naturally occurring differences as a basis for differences in the participants' comparison sets. Results from all four studies confirmed our hypothesis that the prevalence of a feature in different comparison categories changes people' estimations of the feature's prevalence in novel categories. These results, highlighting how context-sensitive interpretations of generic language are to listeners' prior knowledge, suggest a possible source for *well-intentioned* miscommunications, where conversational partners are cooperative during a discourse but are led by their different backgrounds to make dissimilar inferences of the same statement. 
    
keywords: >
    generics; semantics; meaning; learning; Bayesian inference
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(tinytex)
library(png)
library(jpeg)
library(grid)
library(xtable)
library(tidyverse)
library(lubridate)
library(lme4)
library(lmerTest)
library(knitr)
library(tidyboot)
library(ggrepel)
library(ggridges)
library(here)
library(broom)
library(broom.mixed)
library(viridis)
library(papaja)
library(ggthemes)

theme_set(theme_classic(base_size = 10))

options(digits=2)
```

# Introduction

Generic language like "birds lay eggs" is a simple, highly frequent way of transmitting information in everyday speech [@gelman1998;@gelman2008]. Generics are distinct from statements about particular referents "e.g. that bird lays eggs"; they transmit information about *categories*. Indeed, a large body of research has documented the power of generic language in adults' and children's inference about familiar and novel categories [e.g.,@cimpian2010;@cimpian2011;@rhodes2012]. Despite their ubiquity, generic statements defy a straightforward definition in threshold semantics (i.e. they do not specify a fixed prevalence rate). While people generally agree that "birds lay eggs," this does not mean that "all birds lay eggs ($100\%$)" nor does it mean that "most birds lay eggs ($>50\%$)"--male birds, and young female birds do not. Similarly, "birds lay eggs" cannot mean "some birds lay eggs ($>0\%$)," because it is true while "birds are female" is not.

Recent work from @tessler2019 shows that generics can be understood through the lens of Gricean pragmatic inference [@grice1975]. Their key insight is that generics can be interpreted as statements about relative prevalence. If a speaker makes a vague statement like "birds lay eggs," but listeners assume that they are cooperatively intending to be informative, they can infer that the speaker means something like "birds are *more likely than you would have expected* to lay eggs." This formulation leaves open two questions: (1) how much more likely does a speaker mean, and (2) what did the listener expect? Tessler and Goodman answer the first question by showing that listeners do not need to resolve this ambiguity directly, but can instead integrate over all prevalence rates that would make the speaker's statement true. In a series of experiments with both familiar and novel generics, @tessler2019 show that people's judgements about prevalence rates following a generic statement are described by a rational model of pragmatic inference [@frank2012]. 

We take up the second question: How do listeners arrive at their prior expectations? One possibility is that implicit in a generic statement is a set of reference categories, i.e. "birds lay eggs" means "relative to relevant comparison categories, birds are more likely to lay eggs." The listener's interpretation of a generic, then, should depend on the set of categories they consider relevant. That is, "feps are friendly (relative to puppies)" should lead to a much different estimate of the prevalence of friendliness in feps than "feps are friendly (relative to squirrels)." 

We test this prediction in a series of four experiments in which people learn about novel categories through generic language. In the first two experiments, we manipulate the implied comparison category directly and show that people's judgments about the prevalence of a feature in a novel category tracks the prevalence level of the implied category. In Experiments 3 and 4, we show the influence of implicit comparison categories without manipulating them. Here we leverage prior work showing that people's estimates about the prevalence of preferences and beliefs in others are egocentrically biased towards the prevalence of those preferences and beliefs in their local communities [@ross1977]. Together, these studies highlight the fundamentally relative way in which even simple generic statements are interpreted, and point towards a potential source of misunderstanding and errors in learning that can arise from well-intentioned communication. 

# Experiment 1

In this experiment, we asked participants to make prevalence judgments for familiar category-feature pairs. Partcipants' prevalence estimations will be used as a measure of baseline prevalence ratings for each feature in a given category. These familiar categories will be provided to a different group of participants in Experiment 2 as comparison categories.

```{r e1_data}
#### read in data
### baseline ###
data_baseline <- read_csv(here("baseline-survey110218/data/020818baseline-anonymized-data.csv"))
### explicit generic ###
data_generic <- read_csv(here("generic-survey110218/data/020818generic-anonymized-data.csv"))

#### filter participants who passed attention check 
### baseline ###
qualtrics_filtered_baseline <- data_baseline %>%
  filter(Q8 == "friendly,tasty,heavy")
### explicit generic ###
qualtrics_filtered_generic <- data_generic %>%
  filter(Q10 == "kobas,feps,dands")

#### Munge data
### In both surveys, the categories and features we chose are consistent.
# use to group different levels
high_objects <- c("puppies","trucks","pizzas")
med_objects <- c("goats", "rocks", "fruits")
low_objects <- c("squirrels", "bikes", "vegetables")
features <- c("friendly", "heavy", "tasty")

# features and objects for the survey
objects <- data_frame(type = "high", object = high_objects,
                      feature = features) %>%
  bind_rows(data_frame(type = "medium", object = med_objects,
                       feature = features)) %>%
  bind_rows(data_frame(type = "low", object = low_objects,
                       feature = features))

# get data for each of the three conditions
### baseline ###
q1_baseline <- qualtrics_filtered_baseline %>%
  select(id, L_feature, L_object, Q1, Q2_1) %>%
  rename(feature = L_feature, baseline_object = L_object, truefalse_response = Q1, percent_response = Q2_1)

q2_baseline <- qualtrics_filtered_baseline %>%
  select(id, M_feature, M_object, Q3, Q4_1) %>%
  rename(feature = M_feature, baseline_object = M_object, truefalse_response = Q3, percent_response = Q4_1)

q3_baseline <- qualtrics_filtered_baseline %>%
  select(id, H_feature, H_object, Q5, Q6_1) %>%
  rename(feature = H_feature, baseline_object = H_object, truefalse_response = Q5, percent_response = Q6_1)

### explicit generic ###
colnames(qualtrics_filtered_generic)[colnames(qualtrics_filtered_generic) == "Duration (in seconds)"] <- "time_spent"

# get data for each of the three conditions
q1_generic <- qualtrics_filtered_generic %>%
  select(id, L_feature, L_comparison, L_novel, Q2_1, time_spent) %>%
  rename(feature = L_feature, baseline_object = L_comparison, novel_object = L_novel, percent_response = Q2_1)

q2_generic <- qualtrics_filtered_generic %>%
  select(id, M_feature, M_comparison, M_novel, Q5_1, time_spent) %>%
  rename(feature = M_feature, baseline_object = M_comparison, novel_object = M_novel,percent_response = Q5_1)

q3_generic <- qualtrics_filtered_generic %>%
  select(id, H_feature, H_comparison, H_novel, Q6_1, time_spent) %>%
  rename(feature = H_feature, baseline_object = H_comparison, novel_object = H_novel, percent_response = Q6_1)

# combine
### baseline ###
response_data_baseline <- bind_rows(q1_baseline, q2_baseline, q3_baseline) %>%
  mutate(type = if_else(baseline_object %in% high_objects, "high",
                        if_else(baseline_object %in% med_objects, "medium","low"))) %>%
  mutate(type = factor(type, levels = c("low", "medium", "high")))
### explicit generic ###
response_data_generic <- bind_rows(q1_generic, q2_generic, q3_generic) %>%
  mutate(type = if_else(baseline_object %in% high_objects, "high",
                        if_else(baseline_object %in% med_objects, "medium","low"))) %>%
  mutate(type = factor(type, levels = c("low", "medium", "high")))

#### Plot for CogSci
exp1_old_long_data <- response_data_baseline %>%
  mutate(condition = "baseline")

exp1_new_long_data <- response_data_generic %>%
  mutate(condition = "novel")
  
exp1_all_long_data <- bind_rows(exp1_new_long_data, exp1_old_long_data) %>%
  select(-truefalse_response, -time_spent)

exp1_empirical_stats <- exp1_all_long_data %>%
  group_by(condition, type) %>%
  tidyboot_mean(percent_response) 
```

## Method

### Participants.

`r nrow(data_baseline)` participants were recruited on Amazon Mechnical Turk. Each participant gave informed consent at the start of the Experiment and was paid 10 cents in compensation. Participants were excluded from the final sample if they did not pass an attention check at the end of experiment (`r nrow(data_baseline) - nrow(qualtrics_filtered_baseline)`), yielding a final sample of `r nrow(qualtrics_filtered_baseline)` participants. 

### Design and Procedure.

Three features (friendly, tasty, and heavy) were chosen, and, for each feature, we chose three categories that were relevant to the feature that will elicit different levels(low, medium, high) of prevalence estimation. Every participant answered questions about each of three features and, for each feature, one randonly-selected category. The order in which the features appeared in the survey was randomized, and each participant was tested on only one category from each of the predetermined prevalence levels.

Participants were first shown a generic for each of the category and feature pair. Then they were asked to first evaluate the truth condition of the generic by answering a forced-choice True or False question, and then to estimate the proportion of the feature within the given category. Participants gave their responses to the estimation question on a scale slider, ranging from 0% to 100%.

After completing these questions, participants were given an attention check to ensure that they had read and engaged with the stimuli. Here, we asked participants to choose the three features we asked about in the survey from a larger set of features.

## Results and Analysis

```{r}
response_filtered_yes_baseline <- response_data_baseline %>%
  filter(truefalse_response == "Yes") %>%
  mutate(true = "baseline_yes")

empirical_stats_filtered_yes_baseline <- response_filtered_yes_baseline %>%
  group_by(type) %>%
  tidyboot_mean(percent_response)

exp1_baseline_lmer <- exp1_old_long_data %>%
  mutate(id = paste0(condition, "_", id)) %>%
  lmer(percent_response/100 ~ type * truefalse_response + (1|id) + (1|feature), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group)

exp1_baseline_all_lmer <- exp1_old_long_data %>%
  mutate(id = paste0(condition, "_", id)) %>%
  lmer(percent_response/100 ~ type + (1|id) + (1|feature), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group)

```


```{r}
exp1_lmer <- exp1_all_long_data %>%
  mutate(id = paste0(condition, "_", id)) %>%
  lmer(percent_response/100 ~ type * condition + (1|id) + (1|feature), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group)
```

Participants' mean prevalence judgments about the target feature in each category increased as predicted from low to medium to high (as shown by the baseline condition in Figure \ref{fig:exp1_fig}). We confirmed this prediction statistically using a mixed-effects logistic regression, predicting the participants' judgments from prevalence level, with random effects of subject and feature (\texttt{prop $\sim$ level + (1|subj) + (level|feature)}). This model revealed a significant effect of level, with both medium ($\beta =$ `r exp1_baseline_all_lmer %>% filter(term == "typemedium") %>% pull(estimate) %>% round(2)`, $t =$ `r exp1_baseline_all_lmer %>% filter(term == "typemedium") %>% pull(statistic) %>% round(2)`, $p$ `r exp1_baseline_all_lmer %>% filter(term == "typemedium") %>% pull(p.value) %>% printp()`) and high ($\beta =$ `r exp1_baseline_all_lmer %>% filter(term == "typehigh") %>% pull(estimate) %>% round(2)`, $t =$ `r exp1_baseline_all_lmer %>% filter(term == "typehigh") %>% pull(statistic) %>% round(2)`, $p$ `r exp1_baseline_all_lmer %>% filter(term == "typehigh") %>% pull(p.value) %>% printp()`) levels of *a priori* prevalence receiving higher prevalence judgments.

Furthermore, the number of participants who evaluated the generic as true also varied across conditions ($n_{low}=$ `r empirical_stats_filtered_yes_baseline %>% filter(type=="low") %>% pull(n)`, $n_{medium}=$ `r empirical_stats_filtered_yes_baseline %>% filter(type=="medium") %>% pull(n)`, $n_{high}=$ `r empirical_stats_filtered_yes_baseline %>% filter(type=="high") %>% pull(n)`), with an increased number of participants evaluating true for generics that contained categories from higher prevalence levels (medium and high). Participants who evaluated the generic as true for categories from the low prevalence level also on average made higher estimation of prevalence ($\mu_{low}=$ `r empirical_stats_filtered_yes_baseline %>% filter(type=="low") %>% pull(mean) %>% round(2)`, $CI_{lower}=$ `r empirical_stats_filtered_yes_baseline %>% filter(type=="low") %>% pull(ci_lower) %>% round(2)`, $CI_{upper}=$ `r empirical_stats_filtered_yes_baseline %>% filter(type=="low") %>% pull(ci_upper) %>% round(2)`) comparing to all participants for categories in the same prevalence level($\mu_{low}=$ `r exp1_empirical_stats %>% filter(condition=="baseline") %>% filter(type=="low") %>% pull(mean) %>% round(2)`, $CI_{lower}=$ `r exp1_empirical_stats %>% filter(condition=="baseline") %>% filter(type=="low") %>% pull(ci_lower) %>% round(2)`, $CI_{upper}=$ `r exp1_empirical_stats %>% filter(condition=="baseline") %>% filter(type=="low") %>% pull(ci_upper) %>% round(2)`). We then used a mixed-effect logistic regression to predict participants' judgments from prevalence level and their true or false evaluation of the generic as well as their interaction, with random effects of subject and feature (\texttt{prop $\sim$ level * response + (1|subj) + (level|feature)}). This model revealed a significant effect of the true or false evaluation, with the true evaluation($\beta =$ `r exp1_baseline_lmer %>% filter(term == "truefalse_responseYes") %>% pull(estimate) %>% round(2)`, $t =$ `r exp1_baseline_lmer %>% filter(term == "truefalse_responseYes") %>% pull(statistic) %>% round(2)`, $p$ `r exp1_baseline_lmer %>% filter(term == "truefalse_responseYes") %>% pull(p.value) %>% printp()`) leading to higher prevalence judgements, a significant effect of the aprior high prevalence level ($\beta =$ `r exp1_baseline_lmer %>% filter(term == "typehigh") %>% pull(estimate) %>% round(2)`, $t =$ `r exp1_baseline_lmer %>% filter(term == "typehigh") %>% pull(statistic) %>% round(2)`, $p$ `r exp1_baseline_lmer %>% filter(term == "typehigh") %>% pull(p.value) %>% printp()`) receiving higher prevalence judgments, and a significant interaction between the true evaluation and high prevalence level ($\beta =$ `r exp1_baseline_lmer %>% filter(term == "typehigh:truefalse_responseYes") %>% pull(estimate) %>% round(2)`, $t =$ `r exp1_baseline_lmer %>% filter(term == "typehigh:truefalse_responseYes") %>% pull(statistic) %>% round(2)`, $p$ `r exp1_baseline_lmer %>% filter(term == "typehigh:truefalse_responseYes") %>% pull(p.value) %>% printp()`). 


## Discussion

The results from Experiment 1 confirmed our hypothesis that participants' estimations of feature prevalence for a category in a given generic varied by the predetermined prevalence level of comparison categories. Participants who evaluated true for the generics gave similar estimations on avaerage for categories from low and medium prevalence levels while providing a significantly higher mean prevalence rating for the categories in the high prevalence level. One possibility is that participants who agreed with the generic are more likely to give a relatively higher prevalence ratings, resulting in similar mean prevalence estimations for categories from the low and medium prevalence levels. However, participants' prevalence judgmenet were still highly sensitive to the comparison category (from aprior low and medium prevalence levels or the high prevalence level) rather than a fixed threshold. Furthermore, the chosen category-feature pairs successfully elicited participants' responses across diffferent prevalence levels. Next, in Experiment 2, we introduced participants to novel categories along with the same set of familiar comparison categories and features. We predict that participants' estimation of a given feature in a novel category will be sensitive to its respective familiar comparison category and the aprior prevalence levels.


# Experiment 2

Experiment 2 included a novel category survey, where participants were introduced to a novel category along with a familiar comparison category, then they were shown a generic containing a novel category and a familiar feature and asked to estimate the prevalence rate of the feature within the novel category.

## Method

### Participants.

`r nrow(data_baseline)` participants were recruited through Amazon Mechanical Turk. Each participant gave informed consent at the start of the Experiment and was paid 10 cents in compensation. Participants were excluded from the final sample if they did not pass an attention check at the end of experiment (`r nrow(data_generic) - nrow(qualtrics_filtered_generic)`), yielding a final sample of `r nrow(qualtrics_filtered_generic)` participants.

### Design and Procedure.

The same sets of features and categories from Experiment 1 were included in Experiment 2. Additional, three novel categories were introduced (one novel category per feature). Participants answered similar questions as the baseline survey in Experiment 1 but about novel categories. Participants were first told that they are visiting three new countries, and people from there will introduce them to things in their respective countries. Each novel category was first introduced by reference to one of the baseline categories from Experiment 1 (e.g. "Feps are like puppies"). Then, they were given a novel generic using the same features about this novel category (e.g., "Feps are friendly"). All other aspects of the design were identical.

After completing these questions, participants were given an attention check to ensure that they had read and engaged with the stimuli. We asked participants to select the three novel categories that were mentioned in the survey.

## Results and Analysis


```{r exp1_fig, fig.env="figure", fig.align = "center", fig.width=3, fig.height=2.5, fig.cap = "Prevalence judgments of participants in the Baseline (1a) and Novel (1b) conditions. Error bars indicate 95\\% confidence intervals computed by nonparametric bootstrapping", cache = T}
exp1_empirical_stats %>% 
  ggplot(aes(x = type, y = empirical_stat, fill = condition)) +
  geom_bar(stat="identity", position=position_dodge()) +
  geom_linerange(aes(ymin=ci_lower, ymax=ci_upper), position = position_dodge(.9))+
  scale_fill_ptol() +
  xlab("Predicted prevalence Level") +
  ylab("Prevalence judgment (%)") +
  theme(legend.position = c(.2,.9), legend.title = element_blank())
  #labs(title = "Estimation Responses to Selected Questions Across Features", subtitle = "Respective comparison category assigned to participants in the novel condition") + 
  # theme(axis.title.x =element_text(size=8, hjust = 0.5),
  #       axis.title.y =element_text(size=8, hjust = 0.5),
  #       legend.text = element_text(size=6),
  #       legend.title = element_text(size = 8))
```

Figure \ref{fig:exp1_fig} shows participants' mean prevalence judgments across conditions in both conditions. In both Experiments, participants' judgments about the prevalence of the target feature in each category increased as predicted from low to medium to high. In addition, judgments made about the novel category were on average higher than the judgements for the corresponding category made by participants in the baseline condition, although this difference was not apparent in the high prevalence condition. We confirmed these predictions statistically using a mixed-effects logistic regression, predicting participants' judgments from condition and prevalence level and their interaction, with random effects of subject and feature (\texttt{prop $\sim$ condition * level + (1|subj) + (level|feature)}). This model revealed a significant effect of level, with both medium ($\beta =$ `r exp1_lmer %>% filter(term == "typemedium") %>% pull(estimate) %>% round(2)`, $t =$ `r exp1_lmer %>% filter(term == "typemedium") %>% pull(statistic) %>% round(2)`, $p$ `r exp1_lmer %>% filter(term == "typemedium") %>% pull(p.value) %>% printp()`) and high ($\beta =$ `r exp1_lmer %>% filter(term == "typehigh") %>% pull(estimate) %>% round(2)`, $t =$ `r exp1_lmer %>% filter(term == "typehigh") %>% pull(statistic) %>% round(2)`, $p$ `r exp1_lmer %>% filter(term == "typehigh") %>% pull(p.value) %>% printp()`) levels of apriori prevalence receiving higher prevalence judgments, a significant effect of condition ($\beta =$ `r exp1_lmer %>% filter(term == "conditionnovel") %>% pull(estimate) %>% round(2)`, $t =$ `r exp1_lmer %>% filter(term == "conditionnovel") %>% pull(statistic) %>% round(2)`, $p$ `r exp1_lmer %>% filter(term == "conditionnovel") %>% pull(p.value) %>% printp()`), and a significant interaction between the two for both medium (($\beta =$ `r exp1_lmer %>% filter(term == "typemedium:conditionnovel") %>% pull(estimate) %>% round(2)`, $t =$ `r exp1_lmer %>% filter(term == "typemedium:conditionnovel") %>% pull(statistic) %>% round(2)`, $p =$ `r exp1_lmer %>% filter(term == "typemedium:conditionnovel") %>% pull(p.value) %>% printp()`) and high levels ($\beta =$ `r exp1_lmer %>% filter(term == "typehigh:conditionnovel") %>% pull(estimate) %>% round(2)`, $t =$ `r exp1_lmer %>% filter(term == "typehigh:conditionnovel") %>% pull(statistic) %>% round(2)`, $p$ `r exp1_lmer %>% filter(term == "typehigh:conditionnovel") %>% pull(p.value) %>% printp()`) indicating that the change in prevalence levels was largest for the lowest a priori level.

## Discussion

The results from Experiments 1 and 2 confirmed our hypothesis that participants judgements about the prevalence of a feature in a novel category tracks the prevalence level of the implied category. The difference in estimation on average between baseline and novel conditions was significant for both the *a prior* low and medium levels, with the novel conditions' estimation higher than the baseline condition, while the differences between the two conditions for the high prevalence level was not apparent. One possible explanation for the small difference may be that there exists a ceiling effect for the estimations of high prevalence categories, considering that the upper limit for any prevalence rating was 100%. 

Next, in Experiments 3 and 4, we further explored the effect of differences in comparison set without providing participants explicit comparison categories. We instead used naturally occurring differences among participants by collecting participants' demographic information. First, we obtained baseline prevalence estimations of a group of features for different demographic groups in Experiment 3, and showed that the responses in Experiment 3 predicted the prevalence judgements about novel generics in Experiment 4.


# Experiment 3

In Experiment 3, we collected prevalence judgments for a set of 15 habits, activities, and preferences along with demographic information. Our goal was to use this set of questions to find features on which prevalence rate estimates varied across demographic groups. 

## Method

```{r Exp4-data}

data_new <- read_csv(here("/demographic_generic/novel_generic/data/01102019_people_novel_habit_anonymized_data.csv"))
data_old <- read_csv(here("/demographic_generic/baseline-people-habits/data/10312018_people_habit_anonymized_data.csv"))

# filter by condition: passing attention check
old_attnCheckAnswers <- c('drink coffee','cook at home', 'go to the gym', 'drive to work')

old_qualtrics_filtered <- data_old %>%
    rowwise() %>%
  mutate(attention_check = (strsplit(attention_check, ','))) %>%
  mutate(attnTrue = sum(attention_check %in% old_attnCheckAnswers),
         attnFalse= sum(!attention_check %in% old_attnCheckAnswers))%>%
  filter(attnFalse==0)

old_tidy_data <- old_qualtrics_filtered %>%
  filter(gender != "Other/Non-conforming") %>%
  mutate(gender = factor(gender, levels = c("Female", "Male"), labels = c("female", "male")))

old_filtered_data <- old_tidy_data %>%
  filter(age < 60) %>%
  mutate(age_bin = if_else(age > median(old_tidy_data$age), "older", "younger"),
         politics_bin = if_else(politics_1 > median(old_tidy_data$politics_1), 
                                "conservative", "liberal"))

new_attnCheckAnswers <- c('like big cities','cook at home', 'go to the gym', 'consume dairy products')

new_qualtrics_filtered <- data_new %>%
    rowwise() %>%
  mutate(attention_check = (strsplit(attention_check, ','))) %>%
  mutate(attnTrue = sum(attention_check %in% new_attnCheckAnswers),
         attnFalse= sum(!attention_check %in% new_attnCheckAnswers))%>%
  filter(attnTrue ==4 & attnFalse==0)

new_tidy_data <- new_qualtrics_filtered %>%
  filter(gender != "Other/Non-conforming") %>%
  mutate(gender = factor(gender, levels = c("Female", "Male"), labels = c("female", "male"))) 

new_filtered_data <- new_tidy_data %>%
  filter(age < 60) %>%
  mutate(age_bin = if_else(age > median(old_tidy_data$age), "older", "younger"),
         politics_bin = if_else(politics_1 > median(old_tidy_data$politics_1), 
                                "conservative", "liberal"))

old_long_data <- old_filtered_data %>%
  select(big_cities_1, computer_preference_1, cook_at_home_1,own_homes_1, dairy_products_1, go_to_gym_1, gender, age_bin, politics_bin, id) %>%
  gather(question, response, big_cities_1, computer_preference_1, cook_at_home_1,own_homes_1, dairy_products_1, go_to_gym_1)

new_long_data <- new_filtered_data %>%
  select(big_cities_1:go_to_gym_1, 
         gender, age_bin, politics_bin, id)%>%
  gather(question, response, big_cities_1:go_to_gym_1)

#### cor
new_data <- new_long_data %>% 
  group_by(id) %>%
  mutate(response = scale(response)) %>%
  group_by(gender, age_bin, politics_bin, question) %>%
  summarise(mean = mean(response, na.rm = T)) %>%
  mutate(data = "new")

old_data <- old_long_data %>%
  group_by(id) %>%
  mutate(response = scale(response)) %>%
  group_by(gender, age_bin, politics_bin, question) %>%
  summarise(mean = mean(response)) %>%
  mutate(data = "old")

all_data <- bind_rows(new_data, old_data) %>%
  group_by(question) %>%
  spread(data, mean) %>%
  summarise(cor = cor(new, old, use = "complete"))

t_test_cor_result <- t.test(all_data$cor)

# UNSCALED
unscaled_new_data <- new_long_data %>% 
  group_by(id) %>%
  group_by(gender, age_bin, politics_bin, question) %>%
  summarise(mean = mean(response, na.rm = T)) %>%
  mutate(data = "new")

unscaled_old_data <- old_long_data %>%
  group_by(id) %>%
  group_by(gender, age_bin, politics_bin, question) %>%
  summarise(mean = mean(response)) %>%
  mutate(data = "old")

unscaled_all_data <- bind_rows(unscaled_new_data, unscaled_old_data) %>%
  group_by(question) %>%
  spread(data, mean) %>%
  summarise(cor = cor(new, old, use = "complete"))

unscaled_all_data_mean <- bind_rows(unscaled_new_data, unscaled_old_data) %>%
  group_by(question) %>%
  spread(data, mean)

all_data_mean <- bind_rows(new_data, old_data) %>%
  group_by(question) %>%
  spread(data, mean)

question_labels <- c(big_cities_1 = "like big cities",
                     computer_preference_1 = "prefers Macs over PCs",
                     cook_at_home_1 = "like to cook at home",
                     dairy_products_1 = "consume dairy products",
                     go_to_gym_1 = "go to the gym",
                     own_homes_1 = "own homes")
```

### Participants.

`r nrow(data_old)` participants were recruited on Amazon Mechanical Turk. Each participant gave informed consent at the start of the experiment and was paid 50 cents in compensation. Participants were excluded from the final sample if they did not pass an attention check at the end of experiment. `r nrow(data_old) - nrow(old_qualtrics_filtered)` were excluded for failing the attention check. We further excluded participants who self-identified as gender non-conforming (`r nrow(old_qualtrics_filtered) - nrow(old_tidy_data)`) or participants over 60-year-old (`r nrow(old_tidy_data) - nrow(old_filtered_data)`) as the data for these two categories were too sparse for our purpose of analysis, yielding a final sample of `r nrow(old_filtered_data)` participants ($M_{age} =$ `r round(mean(old_filtered_data$age), digits=0)`, $sd =$ `r round(sd(old_filtered_data$age), digits=2)`).

### Design and Procedure.

Fifteen features were chosen to be included in the survey. Each feature was either a familiar habit or activity people might participate in their daily lives (e.g., like to cook at home, go to the gym, consume dairy products). Every participant was asked to estimate what proportion of people who engaged in that particular habit or activity. The order in which the features appeared in the survey was randomized. After making these judgments, participants were asked to report their age, gender, political ideology score, and zip code. Participants recorded their responses for the estimation question on a scale slider, ranging from 0% to 100%. For the collection of demographic information, participants typed their age in a text box, selected one choice from either "female", "male", or "other/non-conforming" for the gender question, recorded their political ideology score on a slider ranging from 1 (most liberal) to 7 (most conservative), and typed their zip code in a text box. After completing these questions, participants were given an attention check to ensure that they had read and engaged with the stimuli. We asked participants to select four features we asked about in the survey.


## Results and Analysis

Although we collected a large number of judgments (10,890), we struggled to detect reliable demographic difference in judgments due to non-independence of group membership that made estimation of individual group effects difficult (e.g men were more likely to be conservative). However, we were underpowered to treating each conjunction of groups (e.g. male, conservative, older) as an independent demographic level. Further, people in different demographic groups had different biases in the use of the 0% to 100% scale (e.g. men gave higher values for every question). Nevertheless, we wanted to find a subset of the most demographically variable questions for use in Experiment 4. We decided that from the total set of 15, we would choose the 6 for which there was the most reliable variability across demographic groups. In order to find the best subset, we turned to simulation. 

For each step of the simulation, we would first select 6 of the 15 questions. We divided our data randomly into two halves, and computed the mean judgement for each question given by each conjoined demographic group in each half (e.g. men gave higher values for every target question). We then scaled the responses in each group across question to remove biases in scale use, and finally computed the correlations in demographic groups' judgments for each individual question across the two halves of the data. We computed this correlation for 100 random samples for each possible combination of 6 of 15 questions. We then chose the set of 6 questions for which the correlations across two random splits of the corpus was the highest. Figure \ref{fig:Exp2-ridges} shows two questions that were characterized by this stable regularity. These features show both the biases in scale use that these demographic groups brought to the task (e.g. female older liberals gave high values in general), but also the relative variability across questions (relative to their judgments about what proportion of people go to the gym, female young conservatives give lower values than we would expect given the values they typically give).

Because we developed this simulation method after having observed the data, we pre-registered this set of questions and a full analysis plan on the Open Science Framework. We predicted that a new set of participants, divided into the same demographic groups, would give judgments about the prevalence of these 6 habits and activities in a novel group of people that was correlated with their judgments about people they knew.


```{r Exp2-ridges, fig.env = "figure", fig.width = 3.5, fig.cap = "Prevalence judgments of two questions by demographic groups in baseline survey"}
old_long_data%>%
  filter(question %in% c("go_to_gym_1", "own_homes_1")) %>%
  mutate(demographics = paste(gender, age_bin, politics_bin, sep = ", \n")) %>%
  mutate(demographics = fct_reorder(demographics, response)) %>%
  ggplot(aes(y=demographics, x=response, fill=demographics)) +
  geom_density_ridges(alpha=0.6, bandwidth=6, scale=1.5) +
  scale_fill_viridis(discrete=TRUE) +
  scale_color_viridis(discrete=TRUE) +
  xlab("Responses (%)") +
  ylab("") +
  coord_fixed(ratio = 15, xlim = c(0, 100))+
  facet_wrap(~question, labeller=labeller(question = question_labels)) +
  theme(axis.title.x =element_text(size=6, hjust = 0.5),
        axis.title.y =element_text(size=4, hjust = 0.5),
        strip.text.x = element_text(size = 6),
        plot.title = element_text(size = 8),
        legend.position="none",
        panel.spacing = unit(0.3, "lines"),
        strip.text.y = element_text(size = 4),
        axis.text.y = element_text(size = 4, vjust = -0.25))
```

# Experiment 4

## Method
Experiment 4 used the simulation result in Experiment 3 to finalize a set of 6 questions to be asked in a novel category survey. In the novel category survey, participants were introducted to six novel categories, then they were shown a generic containing a novel category and a familiar feature, and asked to make prevalence estimations for each habit among people in the foreign countries. Participants were asked to provide some demographic information, including their gender, age, political ideology score, and zip code.


```{r exp4-map, fig.pos = "H", fig.align='center', fig.width=3, fig.heighti=3, set.cap.width=T, fig.cap = "Novel category survey map prompt"}

img_map <- readJPEG(here("cogsci_paper/figs/novel_country_map3.jpeg"))
grid::grid.raster(img_map)

```

### Participants.

`r nrow(data_new)` participants were recruited on Amazon Mechanical Turk. Each participant gave informed consent at the start of the exerpiment and was paid $0.2$ dollar in compensation. Participants were excluded from the final sample if they did not pass an attention check at the end of experiment. `r nrow(data_new) - nrow(new_qualtrics_filtered)` were excluded for failing the attention check. We further excluded participants who self-identified as gender non-conforming (`r nrow(new_qualtrics_filtered) - nrow(new_tidy_data)`) or participants over 60-year-old (`r nrow(new_tidy_data) - nrow(new_filtered_data)`), yielding a final sample of `r nrow(new_filtered_data)` participants ($M_{age} =$ `r round(mean(new_filtered_data$age), digits=0)`, $sd =$ `r round(sd(new_filtered_data$age), digits=2)`).

### Design and Procedure.

Participants in this condition were first shown a map of 6 imaginary countries and their corresponding novel names (see Figure \ref{fig:exp4-map}) along with a prompt. They were told that people from these six countries will introduce some habits of people from their countries to them. They were then introduced to generic statements about each of the country and a habit selected from the simulation. After reading the generic statement, participants were then asked to make an estimate the percentage of people having a certain habit or activity. Participants responded by choosing a number from 0% - 100% using a slider bar. The order in which the questions appeared was randomized. After completing these questions, participants were given an attention check to ensure that they had read and engaged with the stimuli. We asked participants to select four features that were mentioned in the survey.

```{r exp4-mean,fig.env = "figure", fig.cap = "Mean responses (scaled) for baseline prevalence judgments and novel category prevalence judgment"}
all_data_mean %>%
  ungroup() %>%
  mutate(question = factor(question, labels = c("live in big cities", "prefer macs", "like to cook at home", "consume dairy products", "go to the gym", "own homes")),
         demographics = paste(gender, age_bin, politics_bin, sep = ", \n"),
         plot_demographics = if_else(demographics %in% c("male, \nyounger, \nliberal", "female, \nolder, \nconservative"), demographics, "")) %>%
  ggplot(aes(x = old, y = new, colour = question, label = plot_demographics, group = question)) +
  geom_jitter(size = 0.8) +
  geom_text_repel(size = 1.5, force = 10, point.padding = 0.1, box.padding = 0.3, show.legend = F, segment.alpha = 0.4) +
  #coord_fixed(ratio=1, xlim = c(20, 100), ylim = c(20, 100)) +
  #scale_x_continuous(limits = c(30, 90)) + 
  #scale_y_continuous(limits = c(30, 90)) +
  #geom_abline(intercept = 0, slope = 1, color = "lightgray", linetype = "dashed") + 
  geom_smooth(method = "lm", se = F) + 
  scale_colour_discrete(name="Question") +
  ylab("mean response from novel generics") + 
  xlab("mean response from baseline") +
  # labs(title= "Mean Response of Prevalence Estimation for Questions", subtitle = "What proportion of people(novel category) do you think _______?") +
  theme(axis.title.x =element_text(size=6, hjust = 0.5),
        axis.title.y =element_text(size=6, hjust = 0.5),
        strip.text.x = element_text(size = 4),
        legend.position = c(.18, .85),
        legend.key.height=unit(.75,"line"),
        legend.title = element_blank(),
        legend.text = element_text(size = 4))
```

## Results

We calculated the pearson correlation of average mean responses across demographic groups between Experiments 3 and 4 for each of the 6 questions selected via simulation. The average mean responses between the two experiments are significantly correlated for each question ($p =$ `r t_test_cor_result$p.value %>% printp()`, $t = $ `r t_test_cor_result$statistic`). The highest correlation among the six questions was `r all_data %>% filter(question == "cook_at_home_1") %>% pull(cor) %>% round(2)` for the feature "like to cook at home", and the lowest correlation was `r all_data %>% filter(question == "go_to_gym_1") %>% pull(cor) %>% round(2)` for "go to the gym". We further examined the within-demographic group differences in mean responses between the baseline and novel experiments by questions (see Fig \ref{fig:exp4-mean}) using the scaled data. The demographic groups were divided into 8 subgroups (2 age bins $\times$ 2 gender bins $\times$ 2 political ideology bins). The bins for dividing age and political ideology scores in Experiment 4 were based on the mean age and mean political ideology scores in Experiment 3. Each dot in a group of same coloured dots in Figure \ref{fig:exp4-mean} is a unique demographic group. Using scaled data in Figure \ref{fig:exp4-mean}, the differences between mean responses from the novel generic and from the baseline category was lost. However, for unscaled prevalence judgments, the mean responses from each of the 8 demographic groups for the novel conditions were consistently higher than the mean responses for the baseline condition across all six questions. The results from Experiments 3 and 4 together showed that participants from different demographic groups make different prevalence estimations within each question, and their responses were on average higher for novel conditions than for baseline conditions. Participants across demographic groups also respond to different questions differently.

One of our motivations for using demographic groups as a proxy for different clusters of comparison sets is that people may sample their comparison categories from localized communities, where members of the same demographic groups are more likely to be in vicinity of each other. However, the results from these different demographic groups may also be confounded, since it is more likely that people do have interactions with others who are not exactly in their specific demographic groups (e.g., a female older liberal person interacts with a male younger liberal person in her daily life). Moreover, the results from Experiments 3 and 4 may be confounded with people's ingroup/outgroup biases due to the selection of features, which may trigger people to use their ingroup versus outgroup distinction to make estimations depend on whether they were making judgments on the baseline *people* or the novel category (people from other novel countries). Our results showed that people systematically overestimate activities of others from outgroups upon hearing a novel generic but their estimations make reference to their ingroup beliefs. 


# General Discussion

In Experiments 1 and 2, we showed that people's prevalence judgments of a feature in a novel category were highly correlated with their judgments of the comparison category by directly providing participants the familiar category. Using naturally occurring data, results from Experiments 3 and 4 revealed that across features people from different demographic groups make consistently different prevalence judgments for familiar and novel categories. Moreover, people's prevalence judgments of novel categories were on average higher than their judgments of familiar categories given the same set of features. In sum, these experiments demonstrate the highly context-dependent nature of generic language.

The results from our studies further suggest a source of miscommunication between speakers and listeners due to differences in their prior experiences, despite their cooperative intentions. While in our studies we asked participants to explicitly estimate the prevalence ratings, this kind of distributional information is not always available or frequently exploited by conversational partners during a discourse. Speakers and listeners may not be aware of the asymmetrical interpretations of the same statement due to their prior knowledge. 

Moreover, recent theories of lexical knowledge propose that multiple sources of information contribute to word meanings, and people's interpretation of words do not come context-free but heavily rely on other linguistic and non-linguistic cues, such as grammatical structures and discourse constraints [@elman2009]. The findings from our studies support this approach of understanding lexical knowledge. By showing that people's interpretation of a novel category reflects their background knowledge of familiar categories (absent in the moment of discourse), we provide another piece of compelling evidence that the interpretation of a word is also dependent on listeners' prior experiences which occurred ahead of the moment of discourse. 
Misunderstandings may arise due to differences in speaker's and listener's prior knowledge, which can lead to unintentional transmission of erroneous information. Upon hearing an utterance by a speaker, listeners may generate interpretations that deviate far from speaker's intended meaning. Consider a situation where the speaker may produce a generic about a feature in a category with in mind a certain prevalence rating $x$. While sharing some common background that facilitated their cooperative conversation, due to the differences in their individual experiences, the listener may make an inference about the prevalence of the same feature in the given category to be $y$, where the value of $y$ depends largely on the listener's prior knowledge. The direction of asymmetrical estimations between speaker and listener is also highly dependent on the prior distributional information a listener has access to and can thus base her estimation on (e.g., $x$ can be smaller than $y$, or vice versa.) When the category under discussion is a certain unfamiliar social group and some obscure feature, this asymmetry in listener's interpretation may further facilitate the transmission of stereotyping beliefs. Furthermore, in scenarios where listeners do not have access to other information to correct or update one's understanding, a listener may hold on to an erroneous belief and pass this inaccurate belief to others.

Under the scope of our current studies, we only tested generic expressions that included the bare plural noun phrase, but generic nouns can be singular or plural, with or without an article, have a definite or indefinite article. More work is required to examine whether the relativity in prevalence threshold can be preserved regardless of which specific generic expression one choose. More specifically, one could test whether these different forms of generic expressions are equally capable at eliciting participants' highly context-dependent prevalence judgments. 

Finally, discussions of the theory of context-dependent lexical meaning as well as our empirical results raises theoretically interesting questions about how children interpret generics and possibly learn from them. Child-directed speech is full of generic expressions [@goldin2005;@gelman2008], however, what inferences would children make upon hearing generics that introduce to them a novel category or a novel feature? Are children able to access and utilize comparison sets when they try to interpret a generic? The investigation of these questions may further provide insights on how children learn new concepts utilizing generic expressions as well as the availability of comparison sets for children to draw inference on over development.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
