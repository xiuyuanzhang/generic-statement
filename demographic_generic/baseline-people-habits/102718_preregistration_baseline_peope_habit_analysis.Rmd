---
title: "Preregistration for Baseline People's Habit Survey Data Collection and Analysis"
author: "Xiuyuan Zhang and Dan Yurovsky"
date: "10/27/2018"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This script contains codes that we plan to use for our next collection of data on participants' prevalence estimation of people's habits. 

### Load libraries
```{r load_libraries, message = F, warning = F}
library(tidyverse)
library(lubridate)
library(lme4)
library(DT)
library(ggpirate)
library(gridExtra)
library(knitr)
library(tidyboot)
library(Matrix)
library(effsize)
library(pwr)
library(compute.es)
library(janitor)
library(tidyboot)
theme_set(theme_classic(base_size = 14))
```

### Read in Zip Code - Population Density Data
We read in population density data that helps us map zip code information document collected from participants to density measures. 
```{r zip_data}
zip_data <- read_csv("data/Zipcode-ZCTA-Population-Density-And-Area-Unsorted.csv") %>%
  rename(zip = `Zip/ZCTA`, density = `Density Per Sq Mile`)
```
### Read in anonymized data
We read in anonymized data (see `103118_preregistration_process_raw_data.R` for criterion applied to drop certain columns for privacy reasons)
```{r, message = F}
data_novel <- read_csv("data/10312018_people_habit_anonymized_data.csv")
```
The total number of participants we get is `r nrow(data_novel)`. 

### Applying filtering Criterion
#### Pass attention check
We set an attention check question at the end of our survey, asking participants to select four questions out of all questions they've seen previously in our survey. If participants fail the attention check(don't select all four correct questions), we will drop them from our sample. 
```{r, message = F}
attnCheckAnswers <- c('drink coffee','cook at home', 'go to the gym', 'drive to work')

qualtrics_filtered <- data_novel %>%
    rowwise() %>%
  mutate(attention_check = (strsplit(attention_check, ','))) %>%
  mutate(attnTrue = sum(attention_check %in% attnCheckAnswers),
         attnFalse= sum(!attention_check %in% attnCheckAnswers))%>%
  filter(attnFalse==0)

```
Filter peole who passed the attention check, result: n = `r nrow(qualtrics_filtered)` (out of `r nrow(data_novel)` )

#### Gender & Age filtering
Based on previous data, we do not have many turkers select "Other/Non-comforming" for our question on gender information. For this upcoming study, we will filter out Other/Non-conforming from our sample. Moreover, for age, the cut-off for 95 percentile in our previous data samples has been 60 years old. Since data after 60 years old becomes sparse, we will apply a cut-off of 60 years old and only include data for participants 60 years old or younger.

```{r rename, message = F}
tidy_data <- qualtrics_filtered %>%
  filter(gender != "Other/Non-conforming") %>%
  mutate(gender = factor(gender, levels = c("Female", "Male"), labels = c("female", "male")))

tidy_data %>%
  group_by(gender) %>%
  summarise(n = n()) %>%
  kable()

tidy_data %>%
  ggplot(aes(x = age)) + 
  geom_histogram(binwidth = 5, fill = "white", color = "black")

filtered_data <- tidy_data %>%
  filter(age < 60) %>%
  mutate(age_bin = if_else(age > median(tidy_data$age), "older", "younger"),
         politics_bin = if_else(politics_1 > median(tidy_data$politics_1), 
                                "conservative", "liberal")) %>%
  left_join(zip_data) %>%
  ungroup() %>%
  filter(!is.na(density)) %>%
  mutate(dense = factor(density > median(density, na.rm = T),
                         labels = c("sparse", "dense")))

filtered_data %>%
  group_by(gender, age_bin, politics_bin) %>%
  summarise(n = n(), age = mean(age), politics = mean(politics_1)) %>%
  kable()
```
By filtering to participants who selected "Other/Non-conforming" for gender, we lost `r nrow(qualtrics_filtered) - nrow(tidy_data)` people.

By filtering to participants younger than 60 years old, we lost `r nrow(tidy_data) - nrow(filtered_data)` people


### Exploratory Data Analysis
#### Scaling Data
From our previous data, we found that participants from different demographic groups have the tendency to scale their responses differently. Since we want to measure differences in participants' percentage judgement due to possible different background information rather than differences caused by their scaling differences, we will scale participant responses with regard to each of the demographic groups. 

```{r scaling data, message = F, warning = F}
# organize data in long format
long_data <- filtered_data %>%
  select(big_cities_1:drink_coffee_1,gender, age_bin, politics_bin, dense, id) %>%
  gather(question, response, big_cities_1:drink_coffee_1) #%>%
#  gather(demo, status, gender:dense)

# scaling the data
scaled_data <- long_data %>%
  group_by(demo, status) %>%
  mutate(response = scale(response)) %>%
  ungroup()

combinations <- expand.grid(demo = distinct(scaled_data,demo) %>% pull(),
                           question = distinct(scaled_data,question) %>% pull()) %>%
  mutate_all(as.character)

```


```{r}

group_data <- long_data %>%
  group_by(gender, age_bin, politics_bin, dense, id) %>%
 # mutate(response = scale(response)) %>%
  group_by(gender, age_bin, politics_bin, dense, question) %>%
  summarise(mean = mean(response), sd = sd(response), n = n()) %>%
  arrange(question) %>%
  group_by(question) %>%
  arrange(question, mean)

q_data <- group_data %>%
  group_by(question) %>%
  summarise(range = diff(range(mean)), sd = mean(sd)) %>%
  arrange(range)

q_data %>%
  View()

group_data %>%
  View()

big_mer <- long_data %>%
  lmer(response ~ question * gender + question * age_bin + question * politics_bin + 
         question * dense + (1|id), data = .)

summary(big_mer)$coefficients %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  as_data_frame() %>%
  clean_names() %>%
  #select(-df) %>%
  filter(str_detect(rowname, ":")) %>%
 # arrange(desc(abs(t_value))) %>%
  View()

```
#### Analyze effect size
To analyze the effect size of our result, we will apply the following function. For continuous variables, we compute the effect size and power for Pearson correlation at significant level = 0.05, and compute the sample size needed to reach power = 0.8,significant level = 0.05; for discrete variables, we compute the effect size and power using Cohen's d at significant level = 0.05, and compute the sample size needed to reach power = 0.8,significant level = 0.05.
```{r effect size, message = F, warning = F}
# function to calculate the effect size for our data
effect_size <- function(df, d, q) {
  
  sub_df <- df %>%
    filter(question == q, demo == d)
  
  unique_status <- distinct(sub_df, status) %>% pull()
  
  if(length(unique_status) > 2) {
    cor <- cor(sub_df$response, as.numeric(sub_df$status), use = "complete") %>% abs
    es <- res(r = cor, n = nrow(sub_df), verbose = F)
    pw = pwr.r.test(n = nrow(sub_df),r = cor, sig.level = 0.05)
    pw_est = pwr.r.test(r = cor, sig.level = 0.05, power = 0.8)
  } else {
    means <- sub_df %>%
      group_by(status) %>%
      summarise(mean = mean(response), sd = sd(response), n = n()) %>%
      as.data.frame() %>%
      arrange(desc(mean))
    
    es <- mes(means[1, "mean"], means[2, "mean"],
              means[1, "sd"], means[2, "sd"],
              means[1, "n"], means[2, "n"],
              verbose = F)
    # pw = pwr.2p2n.test(h = es$d, n1 = means[1, "n"], 
    #                    n2 =  means[2, "n"], sig.level = 0.05)
    # pw_est = pwr.2p.test(h = es$d, sig.level = 0.05, power = 0.8)
    
  }
 
    data.frame(demographics = d, question = q, d = es$d,
               lower = es$l.d, upper = es$u.d)%>%#, pwr = pw$power, estimate_n_0.8 = pw_est$n ) %>%
      as_data_frame()
}

es <- map2(combinations$demo, combinations$question, 
     ~effect_size(scaled_data, .x, .y)) %>%
  bind_rows() 

q_ranking <- es %>%
  group_by(question) %>%
  summarise(d = mean(d)) %>%
  arrange(d)

d_ranking <- es %>%
  group_by(demographics) %>%
  summarise(d = mean(d)) %>%
  arrange(d)

es %>%
  mutate(question = factor(question, levels = q_ranking$question),
         demo = factor(demographics, levels = d_ranking$demographics)) %>%
  ggplot(aes(x = question, y = d, ymin = lower, ymax = upper, color = demographics)) +
  geom_pointrange(position = position_dodge(.5)) + 
  theme(axis.text.x=element_text(angle=90 ,vjust = .5)) + 
  scale_color_brewer(palette = "Set1")  +
  coord_flip() +
  geom_hline(aes(yintercept = 0), linetype = "dashed")

```

```{r sample effect}

error <- 1

sample_200 <- long_data %>%
  group_by(question) %>%
  sample_n(200, replace = T) %>%
  group_by(gender, age_bin, politics_bin, dense, question) %>%
  summarise(response = mean(response)) %>%
  mutate(response = response + rnorm(n(), 0, error)) %>%
  mutate(type = "sample")

cor_data <- long_data %>%
  group_by(gender, age_bin, politics_bin, dense, question) %>%
  summarise(response = mean(response)) %>%
  mutate(type = "original") %>%
  bind_rows(sample_200) %>%
  spread(type, response) %>%
  group_by(question) %>%
  summarise(cor = cor(original, sample, use = "complete")) %>%
  summarise(cor = mean(cor))


cor_sample <- function() {
  
  half_ids <- long_data %>%
    select(id) %>%
    distinct() %>%
    sample_frac(.25)
  
  long_data %>%
    group_by(id) %>%
    mutate(response = scale(response)) %>%
    ungroup() %>%
    mutate(half = if_else(id %in% half_ids$id, "first", "second")) %>%
    group_by(half, gender, age_bin, politics_bin, dense, question) %>%
    summarise(mean = mean(response)) %>%
    spread(half, mean) %>%
    group_by(question) %>%
    summarise(cor = cor(first, second, use = "complete")) %>%
    summarise(mean = mean(cor, na.rm = T)) %>%
    pull()
}


samples <- replicate(100, cor_sample())

ggplot(samples, aes(x = value)) + 
  geom_histogram() + 
  geom_vline(aes(xintercept = quantile(value, .025))) +
  geom_vline(aes(xintercept = quantile(value, .975)))

cor_byq_sample <- function() {
  
  half_ids <- long_data %>%
    select(id) %>%
    distinct() %>%
    sample_frac(.5)
  
  long_data %>%
    group_by(id) %>%
    mutate(response = scale(response)) %>%
    ungroup() %>%
    mutate(half = if_else(id %in% half_ids$id, "first", "second")) %>%
    group_by(half, gender, age_bin, politics_bin, dense, question) %>%
    summarise(mean = mean(response)) %>%
    spread(half, mean) %>%
    group_by(question) %>%
    summarise(cor = cor(first, second, use = "complete"))
}


samples <- replicate(1000, cor_byq_sample(), simplify = F)

cis <- samples %>%
  bind_rows() %>%
  group_by(question) %>%
  summarise(mean = mean(cor),
            ci_lower = quantile(cor, .025), 
            ci_upper = quantile(cor, .975))

cis %>% arrange(desc(ci_lower)) %>% View()


samples %>%
  bind_rows() %>%
  ggplot(aes(x = cor)) + 
  facet_wrap(~question) +
  geom_histogram()
```

To run
```{r}
#any binary splits, use values from old data
new_data <- long_data %>% #Actually from the new data
  group_by(id) %>%
  mutate(response = scale(response)) %>%
  group_by(gender, age_bin, politics_bin, dense, question) %>%
  summarise(mean = mean(response)) %>%
  mutate(data = "new")

old_data <- long_data %>% # ACtually from this run of data
  group_by(id) %>%
  mutate(response = scale(response)) %>%
  group_by(gender, age_bin, politics_bin, dense, question) %>%
  summarise(mean = mean(response)) %>%
  mutate(data = "old")


all_data <- bind_rows(new_data, old_data) %>%
  group_by(question) %>%
  spread(data, mean) %>%
  summarise(cor = cor(new, old))

t.test(all_data$cor)
```

